\begin{chapter}{\label{cha:conclusion}Conclusion \& Future Work}

In the years since \citet{Wulfsohn97} introduced joint modelling, clinical trials in various disease areas have consistently gathered extensive longitudinal biomarker data. This development has introduced both challenges and opportunities. Multivariate data offers enhanced predictive discrimination, but ignoring its multivariate nature means overlooking potentially informative correlations between longitudinal trajectories. However, traditional parameter estimation methods, which involve multidimensional integrals, present substantial computational and statistical challenges.

This aim of this thesis was to explore alternative approaches for faster fitting of joint models for survival and multivariate longitudinal data. By repurposing an underutilized approximate EM algorithm \citep{Bernhardt15}, we aimed to reduce the computational burden associated with traditional methods of fitting joint models, which successfully lead to faster fitting. Additionally, we moved beyond the commonly restrictive longitudinal specifications used in these models, opting for more flexible and potentially complex specifications.

% OLD -->
% % Introduction, \ie brief summary of what we set out to do -->
% Some twenty-five years after the first introduction of joint modelling in \citet{Wulfsohn97}, clinical trials across multiple disease areas routinely collect information on many longitudinal biomarkers. This represented challenges and opportunities: Multivariate data likely provides better discrimination capabilities from a prediction standpoint, furthermore disregarding the multivariate nature of the data is tantamount to ignoring potentially informative correlations between these longitudinal trajectories; on the other hand, the multidimensional integrals which arise as part of parameter estimation under traditional approaches presented significant computational and statistical difficulties.

% % `One line' summary of thesis -->
% The aim of this thesis was to investigate alternative approaches which enable faster fitting of joint models of survival and \textit{multivariate} longitudinal data. We repurposed an approximate EM algorithm relatively dormant in the literature \citep{Bernhardt15} to lessen the computational burden felt by traditional joint models, leading to faster fitting. Furthermore, we extended beyond the typically-used restrictive longitudinal specifications in such models in favour of flexible, potentially complex specifications.

% Detailed regaling of the thesis ---------------
% Introduction -->
In Chapter \ref{cha:intro} we sought to provide something of a background to joint modelling itself: The data analysis challenges it was borne out of, and early methods it superseded, with particular focus on the seminal article introducing joint modelling as it appears in the thesis \citep{Wulfsohn97}. We then outlined the many ways in which joint models have `evolved' since this first simplified model. For instance, the longitudinal sub-models featuring more complex random effects specifications and/or the presence of more than one longitudinal response. We also drew attention to literature which eschewed imposition of the Gaussian (\ie LMM) -- Survival (\ie Cox PH) meta-model. The joint models as they would appear in Chapters \ref{cha:methods-classic} and \ref{cha:flexible} then having precedence in literature. 

% `Classic' MVJM -->
We then defined multivariate joint models under, what we termed, the `classic' framework in Chapter \ref{cha:methods-classic}, introducing the $K$ linear mixed effects models constructing the longitudinal sub-models as well as the survival sub-model. The observed data likelihood and the form of its constituent densities for such joint models were given, and we outlined semiparametric maximum likelihood via the EM algorithm under a joint modelling framework. With particular focus on the E-step we gave the form of necessary expectations taken against the conditional distribution of the random effects, \ie the multidimensional integration required for parameter updates, outlining numerical techniques to evaluate such integrals appearing in literature. We then detailed a few techniques to obtain standard errors in the parameter estimates \textit{after} convergence of such an EM algorithm. We finally introduced simulation of the longitudinal response, as well as the survival process before combining these and outlining simulation under a joint modelling framework as a whole.

% Approximate EM -->
The EM algorithm outlined in Chapter \ref{cha:methods-classic} serves as our main workhorse for parameter inference in the thesis. In Chapter \ref{cha:approx} we reaffirm our motivation in undertaking multivariate joint model fits (\ie potentially many longitudinal responses and/or random effects), but draw attention to the prohibitive computational expense of existing routines to evaluate multidimensional integrals which may limit implementation of such joint models.\newline
To remedy this we introduce an approximate EM algorithm originally proposed in \citet{Bernhardt15} under a joint model of multivariate longitudinal data with a logistic (in place of the Cox PH model we consider), which allow these intractable multidimensional integrals to be evaluated against a univariate normal distribution.\newline
We outlined starting values, convergence details, as well as the algorithm itself \ie how one achieves convergence. We outlined the parameter updates under this approximate EM approach, followed by extensive simulation studies which aimed to establish the strengths and potential sensitivities of inference by the approximate EM algorithm. We concluded that the algorithm appears to perform well, save for potential issues with scaling with sample size $n$, which is not necessarily exclusive to this line of ML estimation. We closed this chapter by comparisons to established software \citep{Hickey2018}, where we noted very similar parameter estimates which were obtained in a faster manner, particularly for large $K$, and conducted sensitivity analyses.

% Flexible GMVJM -->
Being confident we have an approach under which we can quickly fit multivariate joint models, we began Chapter \ref{cha:flexible} by restating that the `classic' Gaussian assumption imposed on \textit{all} longitudinal responses across Chapters \ref{cha:methods-classic} and \ref{cha:approx} may be inappropriate in clinical settings. We then introduce how one formulates generalised linear mixed models (GLMMs) as well as stating the advantages of such models (\eg we can eschew transformations on data and consider their `most natural' distribution) and preordained uptick in computational demand when electing this modelling avenue.\newline
Attention then focused more specifically on GLMMs within joint modelling: Elucidating the need to accommodate diverse longitudinal outcomes and highlighting the exclusively Bayesian paradigm such joint models are fit under in the literature owing to the form expectations would take if one proceeded by \eg EM instead, before highlighting the usage of the approximation used in Chapter \ref{cha:approx} in circumventing this restrictive formulation. This neatly sets the scene for (what we coin) `generalised' multivariate joint models to be defined.\newline
We consider five candidate exponential families, surplus to the Gaussian, to model the conditional mean of the longitudinal response: Poisson; Gamma; Binomial; Negative binomial; and generalised Poisson. These were chosen in an attempt to capture differing types of data including binary, skewed continuous and counts, including count data exhibiting over- and/or under-dispersion. The densities of these chosen families are presented, and we detail parameter estimation for these generalised multivariate joint models with attention on the fixed effects $\bb$ and dispersion parameters $\bs$; the survival sub-model and random effects specification not being altered here.\newline
As was undertaken in Chapter \ref{cha:approx}, we then carried out simulation studies on these joint models. However, rather than emphasis on data characteristics (with an assumption that the behaviour seen in Chapter \ref{cha:approx} will \textit{not} drastically change), focus was instead placed on demonstration of parameter estimation under mixtures of these different families, in trivariate and five-variate simulations. We also presented univariate simulations for the considered families which housed dispersion parameters. We concluded that the non-Gaussian specifications of the longitudinal sub-models did not adversely affect the parameter estimates; the approximate EM algorithm providing an efficient manner to fit these joint models under maximum likelihood. However, the performance of the algorithm was notably weakest when a binary response was considered, and over-dispersed generalised Poisson, with both shortcomings discussed.\newline 
We closed the chapter by investigating/adapting a pre-existing heuristic for the number of quadrature nodes one should use, where we noted results were in-line with existing literature for joint models, as well as a brief excursion into potential usage of the approximation in a Monte Carlo EM scheme. Across the presented simulation results presented in Chapters \ref{cha:approx} and \ref{cha:flexible}, we identified something of a trade-off between efficient computation and model performance (as determined by \eg 95\% coverage probabilities).

% Normal Justification -->
Taking a step back from these joint models which have dominated the thesis up to this point, we next investigated the normal approximation proffered by \citet{Bernhardt15} and adapted in Chapter \ref{cha:approx} in Chapter \ref{cha:justification}. Here we sought to provide some background to the approximation by drawing attention to similar techniques used in literature previously \citep{Rizopoulos2012} and contrasting with that used in the thesis. We next set out the objectives to be assessed by simulation: Essentially to investigate whether the approximation appears to be reasonable; followed by how we would achieve this. Given a pre-determined set of parameter values, $\bOT$, we sampled from the true posterior $\postb$ and compared this against the approximate normal distribution $\Napprox$. We concluded that the approximation utilised throughout appeared reasonable, even with non-Gaussian conditional distributions on the response. However, we additionally observed the variance-covariance estimate $\hS$ overestimated the variance of the true distribution $\postb$. We hypothesised this could be remedied by future development of a scale factor applied to $\hS$ (\ie to be `closer' to the true normal distribution), or alternative specifications of the random effects altogether.

% Post-hoc analyses -->
With details on parameter estimation and obtention of standard errors given in Chapters \ref{cha:methods-classic}--\ref{cha:flexible}, \ie `having' the fitted joint model, in Chapter \ref{cha:posthoc} we turn attention to a suite of post-hoc analyses one can carry out on the said model. We laid out calculation of residuals for the longitudinal and survival sub-models and provided examples for each. We then surveyed the readily-available inference joint models enjoy with respect to hypothesis testing of the parameters present in the sub-models as well as comparing two nested joint models. However we noted here issues with comparing non-nested models, and that much of the literature here is underdeveloped. We then proceeded from basic inference on a joint model to obtaining predictions following routines in existing literature \citep{Rizopoulos2012}. We outlined estimation of the probabilities of interest, as well as how these can be used to establish time-dependent measures of discrimination and calibration for the fitted joint model, including methods to adequately establish predictive contributions from \eg additional biomarkers.

% Full application to PBC -->
Chapter \ref{cha:app-PBC} acts as an amalgamation of the multivariate joint models introduced in Chapter \ref{cha:methods-classic}, the methods for fast and flexible fitting we developed in Chapters \ref{cha:approx} and \ref{cha:flexible}, supplemented by post-hoc analyses we outlined in Chapter \ref{cha:posthoc}. In this chapter we sought to provide a comprehensive modelling process applied to the motivating set of clinical data, PBC, introduced in Chapter \ref{cha:intro}. We began removed from the joint modelling instead focusing on the longitudinal and survival sub-models, in each case identifying a model which offered a good trade-off between goodness-of-fit and model simplicity. We then established univariate association before considering three separate multivariate joint models, split by broad categories of liver function. Over the next few stages we then arrived at a `final' bivariate joint model. With this in tow, we undertook subject-specific dynamic predictions and presented the associated AUC measures as the focal point. Across model-building stages, as well as in these discriminatory measures, we compared the parameter estimates from the joint models fit by the approximate EM algorithm with existing software, noting good agreement, with approximate EM achieving model convergence in a faster manner.

\section{Future work}
\subsection{Extensions to the survival sub-model}\label{sec:conclusion-future-survival}
% Competing risks -->
Throughout the work presented, we have solely considered a Cox PH model for the time-to-event process in the joint model. One obvious extension would be considering a \textit{cause-specific} (\ie competing risks) model. Examples of joint models with a competing risks survival sub-model include \citet{Williamson2008}, \citet{Li2010} and \citet{Rustand2023}. Such joint models would indeed be more useful in circumstances where patients can experience multiple events of interest: For instance death or disease recurrence; recurrent events such as re-admission to hospital; or a succession of events such as transition between (e.g. worsening of) disease states \citep{Hickey2018B}.

% Accelarated failure times -->
The univariate as well as competing risks survival sub-models are still very much couched within a Cox PH model, an assumption of which being proportional hazards. Although there exist plenty of common techniques to validate this modelling assumption, such a check has not materialised (to the best of the author's knowledge) for joint models. One popular alternative to the Cox PH model are accelerated failure time (AFT) models. Instead of modelling the hazard, AFTs model the time-to-event by either transforming (say taking logarithms of) the survival times, or through imposition of a distributional assumption (\eg Weibull, Gompertz) with extra parameters to be estimated. In instances where the PH model is unappealing due to the assumption of proportional hazards potentially not being met, or some evidence that the survival times follow a specific distribution, a joint model with an AFT sub-model may then be appealing, with two examples being \citet{Tseng2005}, and \citet{Khan2022}.

% Separate \gamma terms
Finally, within the specification of the survival sub-model in the joint modelling framework in \eqref{eq:methods-survival}, one could consider \textit{separate} $\gamma$ terms on the random intercepts and slopes. This would provide inference on \eg risk associated with having a higher than average biomarker measurement at the outset, as well as that associated with quicker than average decline in the biomarker. This specification could be flexibly specified on a per-biomarker basis.

\subsection{Power for joint models}\label{sec:conclusions-future-power}
Throughout the thesis, we referred to consternation surrounding potential lack of power for joint models, with the considerations in Appendix \ref{sec:appendix-sim-considerations-power} being set out for a \textit{univariate} joint model. To the best of the author's knowledge, no real steps have been taken in wider literature to investigate power calculations for the multivariate case. \citet{Riley2019} proffer, under a multivariate Cox PH model, that the model should be fitted to data with a certain sample size $n$ and number of events $\Si\Delta_i$ \textit{relative} to the number of covariates and define their criterion in terms of `events per parameter'. Extending this line of enquiry to a \textit{multivariate} joint model, we may infer that fitting, say, a $K=10$ joint model to a set of data with relatively low sample size and incidence of failure will not have adequate power, and run risk of over-fitting both the estimates $\bg$ as well as the underlying hazard. Investigations herein were outside the scope of the thesis, and remain a vital avenue for future research if multivariate joint models are to be used in practice.

\subsection{Further exponential family members}\label{sec:conclusion-future-families}
Given the focus on GLMMs in Chapter \ref{cha:flexible}, where we consider only five families when modelling the response (excluding the traditional Gaussian), there is a very clear opportunity to allow for yet further flexibility in the specification of the longitudinal sub-model. For continuous responses, the (multivariate) $t_\nu$-distribution (with degrees of freedom $\nu$ to be estimated) could be implemented.\newline In instances where the response is continuous but bounded $\Y\in[0,1]$ \eg representing a probability/percentage then the Beta family could be used to model the conditional mean of such a response. Something of an extension is ordered Beta regression \citep{Ordbetareg} where the interval $[0,1]$ is cut into ordinal segments with some meaningful transition between them; the linear predictor then modelling the cumulative probability of being in each segment.\newline Longitudinal responses which are counts in nature also enjoy a great breadth of potential families. For responses where underdispersion is thought to be present, the Gamma-Count distribution \citep{GammaCount} could be employed, for over- and under-dispersed data the Conway-Maxwell-Poisson (CMP) distribution \citep{ConwayMaxwell, Shmueli2005}, and its reparameterisation via the mean \citep{Huang2017} could be used. This CMP distribution is similar to the generalised Poisson, but with no restriction on its dispersion model.

Outside of the families themselves one could implement, we could additionally consider different `regimes' to each response's sub-model. That is, in addition to the dispersion sub-models we introduced in Section \ref{sec:flexible-gmvjm-disp} we could have modelled other parts of the (sub) modelling process. Perhaps the most obvious avenues here are modelling a structural excess of zeroes (zero-inflation, an example being \citet{Zhu2018}); a structural absence of zeroes (zero-truncation); or hurdle models, which essentially combine these approaches by first modelling the probability of zero versus non-zero counts and then modelling the non-zero counts separately, using a truncated count distribution.

One could therefore view the approximate EM algorithm introduced as being somewhat nascent, with the families we do consider in Chapter \ref{cha:flexible} perhaps providing a basis for a flexible joint modelling approach.

\subsection{Methods for faster computation}\label{sec:conclusion-future-faster}
The approximation in Chapter \ref{cha:approx} reduces computation time by effectively `flattening' multidimensional integrals to one-dimensional ones which are appraised by quadrature. This \textit{alone} doesn't lead to the computation times observed in Chapters \ref{cha:approx}, \ref{cha:flexible} and \ref{cha:app-PBC}, for instance the M-step update for the survival parameters we outlined in Section \ref{sec:approx-Mstep-Phi} still presents a computational bottleneck. This lead to the use of \tt{C++} (interfaced from \tt{R} by \tt{Rcpp} \citep{R-Rcpp} and \tt{RcppArmadillo} \citep{R-RcppArmadillo}) in practicality, which greatly reduced the impact of these bottlenecks as mentioned in Section \ref{sec:approx-implementation} with an example given in Appendix \ref{sec:appendix-gmvjoint-cpp}.

We noted in Section \ref{sec:approx-sims-n} that the approximate EM algorithm appeared to slow down fairly harshly with increased sample size $n$. One promising avenue in literature recently is the use of a linear scan algorithm to greatly improve computational efficiency under semiparametric maximum likelihood via EM \citep{R-FastJM-Paper} One promising avenue in literature recently is the use of a linear scan algorithm to greatly improve computational efficiency under semiparametric maximum likelihood via EM. A linear scan algorithm sequentially processes data points in a single pass, rather than iterating over them all multiple times. This method significantly reduces the computational burden by avoiding the repetitive calculations typically required by EM. By doing so, it enhances the speed and efficiency of its EM algorithm, potentially making it a valuable tool for handling large datasets. This approach is available in \tt{R} package \tt{FastJM} \citep{R-FastJM}. Unfortunately despite the computational promise here, the package is restricted to the univariate Gaussian case, and so wasn't considered in comparisons with other software in Chapters \ref{cha:approx} and \ref{cha:app-PBC}.

The approximate EM algorithm presented in this thesis appears to scale well with many biomarkers, exhibiting a near-linear increase in computation time with $K=2\rightarrow7$, corresponding to an increase in the dimension of the random effects $q=2\rightarrow14$ in Sections \ref{sec:approx-sims-K}, \ref{sec:approx-comparisons}, and \ref{sec:flexible-sim-five}. Despite this, in circumstances where \textit{very} many biomarkers exist in the data, inference can become more complex owing to either the lack of sample size and/or event incidence as we discuss in Appendix \ref{sec:appendix-sim-considerations-power}; we noted the impact in Sections \ref{sec:approx-sims-K} and \ref{sec:approx-comparisons}, furthermore in our application to PBC data in Chapter \ref{cha:app-PBC} we opted for several smaller models in lieu of one large model with this consternation in mind.\newline 
However, should practitioners wish to simply `use all' data, these inferential issues would make the methodology in this thesis inadequate. We could then consider following, for example \citet{Li2017B} and \citet{Li2021}, in use of functional principal components analysis on the longitudinal processes. This could lead to the development of hybrid approaches which utilise some dimension reduction method in tandem with the approximate EM algorithm to use `more' data at the outset, whilst still enjoying faster computation.

We provided either analytical or numerical (see Appendix \ref{sec:appendix-numdiff}) solutions to the differentiation necessary for statistical inference in Sections \ref{sec:approx-Mstep} and \ref{sec:flexible-Mstep-fixef}. One exciting avenue for further development in joint modelling both generally and in terms of computational efficiency could be interfacing with automatic differentiation (AD) via AD Model Builder \citep{ADMB-paper}, implemented in \tt{R} by template model builder (\href{https://kaskr.github.io/adcomp/Introduction.html}{\textcolor{nclblue}{\tt{TMB}}}, \citet{R-TMB}). Without getting into specific details, \tt{TMB} allows the user to near-instantaneously calculate first and second order derivatives for likelihood functions which are either \href{https://kaskr.github.io/adcomp/distributions__R_8hpp_source.html}{\textcolor{nclblue}{pre-existing}}, or user-defined (as a joint model would be), therefore greatly decreasing the time taken per EM iteration.

\end{chapter}